


# Saltzer & Schroeder for 2030: Security engineering principles in a world of AI
https://arxiv.org/pdf/2407.05710


# Saltzer & Schroeder для 2030: Принципи інженерії безпеки у світі штучного інтелекту

## АНОТАЦІЯ

Написання безпечного коду є складним завданням, тому очікується, що після випуску інструментів генерування коду на основі ШІ, таких як ChatGPT та GitHub Copilot, розробники будуть використовувати ці інструменти для виконання завдань безпеки та використання API безпеки. Однак чи є код, згенерований ChatGPT, безпечним? Як звичайний програмний інженер або інженер з безпеки зможе це визначити?

Наближаючись до наступного десятиліття, ми очікуємо більшого впровадження інструментів генерування коду на основі ШІ та бачимо, як розробники використовують їх для написання безпечного коду. Готуючись до цього, нам потрібно забезпечити безпеку за замовчуванням (security-by-design). У цій статті ми озираємося назад у часі на принципи проектування безпеки Saltzer & Schroeder, оскільки вони повинні будуть еволюціонувати та адаптуватися до викликів, які приходять зі світом коду, згенерованого штучним інтелектом.

## КЛЮЧОВІ СЛОВА

Принципи проектування безпеки, ChatGPT, великі мовні моделі

## ПОСИЛАННЯ ACM

Nikhil Patnaik, Joseph Hallett, та Awais Rashid. 2024. Saltzer & Schroeder для 2030: Принципи інженерії безпеки у світі ШІ. У Збірнику Міжнародного семінару з програмної інженерії в 2030 (SE 2030). ACM, Нью-Йорк, Нью-Йорк, США, 5 сторінок. https://doi.org/10.1145/nnnnnnn.nnnnnnn

## 1. ВСТУП

У 1974 році Saltzer розглянув виклик захисту та контролю обміну інформацією в Multics (Multiplexed Information and Computing Service) [26]. Saltzer запропонував 5 принципів проектування для допомоги в оцінці різних рішень. Ці принципи проектування розглядали списки контролю доступу, ідентифікацію та автентифікацію користувачів, ієрархічний контроль специфікацій доступу та захист первинної пам'яті [26].

У 1975 році Saltzer & Schroeder представили 8 принципів проектування та серію бажаних функцій з наміром захистити інформацію, що зберігається на комп'ютері, від несанкціонованого доступу та модифікації. На той час програмні додатки могли зберігати інформацію та одночасно використовуватися декількома користувачами. Ключовим викликом, який хотіли вирішити Saltzer & Schroeder, було питання множинного використання. Для додатків з користувачами, які не мають рівних повноважень, потрібна система для забезпечення бажаної структури повноважень у додатку [27].

Робота Saltzer & Schroeder стала дуже впливовою та застосовною до широкого спектру галузей, таких як забезпечення політик безпеки [28], оцінка архітектури безпеки Java [9] та мінімізація помилок, пов'язаних з користувачами, в безпеці інформаційних систем [30].

Однак лише в 1995 році принципи Saltzer & Schroeder вперше були застосовані до проектування API безпеки через Cryptlib [11]. Між 1995 та 2002 роками Gutmann адаптував роботу Saltzer & Schroeder для вирішення викликів безпеки та зручності використання при проектуванні криптографічного API [11–13]. Робота Gutmann відіграла інтегральну роль у впровадженні принципів проектування Saltzer & Schroeder у галузь проектування API безпеки [1, 10, 19, 23, 24, 32].

Еволюцію принципів Saltzer & Schroeder можна віднести на рахунок постійно змінного ландшафту кібербезпеки та викликів, які з цим приходять, оскільки саме ці виклики змушують принципи еволюціонувати.

Сьогодні ми спостерігаємо настання світу штучного інтелекту. З випуском великих мовних моделей (LLM), OpenAI розпочала гонку ШІ з невизначеною фінішною лінією. Тепер, змагаючись з конкурентами, такими як Google's PaLM та LaMDA, ми бачимо швидкий розвиток у цій галузі. Спільнота розробки програмного забезпечення проявила великий інтерес до цих досягнень, оскільки інтеграція інструментів генерування коду на основі ШІ, таких як ChatGPT та GitHub Copilot, може допомогти їм у багатьох різноманітних завданнях, таких як покращення процесу перегляду коду, надання пропозицій кодування, пропонування виправлень для дефектів у коді.

У майбутньому ці інструменти генерування коду на основі ШІ могли б допомогти в рефакторингу кодових баз великих, складних програмних додатків, покращенні читабельності та полегшенні підтримки з підтримкою документації, згенерованої ШІ [2, 5].

З усіма цими майбутніми змінами в індустрії програмного забезпечення, яку роль відіграють Saltzer & Schroeder у цьому світі ШІ? Чи достатньо їхніх принципів проектування безпеки? Якщо ні, то чому ні? Якщо вони достатні, як ми повинні використовувати їх для формування бачення на 2030 рік?

У цій статті ми оцінюємо здатність ChatGPT безпечно зберігати пароль та оцінюємо згенерований код відповідно до критеріїв Naiakshina для безпечного зберігання паролів [21]. Після цього ми підказуємо ChatGPT повторити завдання, але цього разу використовуючи критерії безпеки Naiakshina як список вимог. Нарешті, ми просимо ChatGPT оцінити свій згенерований код відповідно до принципів проектування безпеки Saltzer & Schroeder [27].

Мета цієї вправи полягає не тільки в розумінні можливостей ChatGPT у безпечному кодуванні та розумінні принципів безпеки, але також як крок у розумінні того, де в динаміці Розробник/ШІ можуть бути впроваджені Saltzer & Schroeder? Де в процесі розробки програмного забезпечення Saltzer & Schroeder були б найбільш ефективними? Якщо вони не ефективні, які недоліки? Чи можуть вони бути вирішені через адаптований набір принципів Saltzer & Schroeder? Чи достатньо Saltzer & Schroeder, чи це кінець дороги?

## 2. ПЕРЕДУМОВИ

### 2.1 Загальні висновки

Після випуску GitHub Copilot, ШІ-програміста для парного програмування, Bird et al. [2] вивчили процес парного програмування та те, як інтеграція GitHub Copilot змінила роль розробника. Bird et al. пояснює, як коли два розробники пишуть код разом, один бере на себе роль водія, пишучи код для призначеного завдання, тоді як інший працює як навігатор, направляючи та переглядаючи роботу водія. Однак Bird et al. виявили, що після впровадження GitHub Copilot роль розробників змінюється від написання коду до розуміння коду. Впровадження GitHub Copilot означає, що навичка перегляду коду стала рівною, якщо не більш важливою, ніж написання коду. Методи, якими розробники можуть оцінювати код, згенерований ШІ, ще не розроблені. Наразі згенерований код проходить той самий процес перегляду, що й код розробника [2, 5].

Щодо продуктивності, GitHub Copilot відомий тим, що допомагає розробникам адаптуватися до різних мов програмування та стилів написання [2]. Однак, що стосується якості коду, Yetiştiren et al. [33, 34] провели емпіричне дослідження для порівняння якості коду та правильності GitHub Copilot, ChatGPT та Amazon CodeWhisperer, використовуючи набір даних HumanEval [4], і виявили, що з точки зору правильності коду ChatGPT показав найкращі результати з правильними рішеннями для 65,2% задач набору даних HumanEval, за ним йде GitHub Copilot. Yetiştiren зазначив, що розробники повинні очікувати знайти деякі помилки з усіма цими інструментами, але ці помилки не настільки поширені, як виявлені "запахи коду" (code smells). Загалом було виявлено 13 "запахів коду" від трьох інструментів генерування коду на основі ШІ [33, 34].

Ernst et al. [5] коментують короткострокові та довгострокові ефекти використання AIDE (Середовища розробки, керовані штучним інтелектом) для вивчення програмування. З одного боку, інструменти як ChatGPT та GitHub Copilot дійсно можуть допомогти початківцю-розробнику вивчити мову програмування, пропонуючи пропозиції та допомагаючи з завданнями кодування. Це може здатися привабливим багатьом розробникам, які приєднуються до індустрії сьогодні, але з іншого боку, ці розробники можуть мати труднощі з повним розумінням того, чому запропонований фрагмент коду працює і як правильно інтегрувати його з їхньою існуючою кодовою базою [5].

Крім того, Codex від OpenAI становить виклик для викладачів та університетських модулів, оскільки він достатньо хороший, щоб перевершити багатьох студентів у модулях "Комп'ютерні науки 1" ступеня комп'ютерних наук. Студенти можуть почати думати, що якщо Codex краще програмує, ніж вони, навіщо їм турбуватися про вивчення програмування? Замість цього студенти почнуть зосереджуватися на інженерії підказок та вивченні того, як налаштовувати код, згенерований ШІ. Їхня роль змінюється від водія (автора коду) до навігатора (дизайнера завдань), але це має серйозні наслідки.

Багато студентів вперше знайомляться з практиками кодування та принципами проектування в університеті, тому впроваджуючи LLM як Codex, студенти можуть бути позбавлені можливості застосувати принципи проектування, які вони вивчають теоретично, до практичних застосувань, оскільки вони можуть попросити ChatGPT згенерувати код для них і втратять свою здатність до вирішення проблем та критичного мислення. Цей потенційний сценарій також зупиняє впровадження та еволюцію принципів проектування безпеки та практик безпечного кодування [3, 6, 7, 22, 27].

У недавньому дослідженні Lau et al. [17] було проведено опитування 20 інструкторів програмування, щоб зрозуміти, як вони планують адаптуватися до впровадження моделей ШІ. На короткострокову перспективу практики мають намір відмовляти від використання таких інструментів, але готуючись до майбутнього, деякі погодилися, що такий опір марний, і готові інтегрувати інструменти генерування коду на основі ШІ з їхніми існуючими програмами.

### 2.2 Наслідки для безпеки використання моделей ШІ

Останні роботи зробили кроки для розуміння наслідків для безпеки використання інструментів генерування коду на основі ШІ [14, 16, 25, 29, 31]. Pearce et al. [25] провели емпіричне дослідження для оцінки продуктивності GitHub Copilot проти підмножини Топ-25 найпоширеніших перерахувань слабкостей (CWE) MITRE. Результати з точки зору безпеки були невизначеними, але Pearce et al. зазначають, що GitHub Copilot навчений на наборі даних відкритого коду, тому розробники повинні очікувати помилки і тому їм радиться залишатися критичними та пильними щодо коду, згенерованого GitHub Copilot. Цікавий момент, згаданий Pearce et al., - це вплив часу на якість безпеки в програмному забезпеченні з відкритим кодом. Найкраща практика на момент написання може повільно стати поганою практикою, оскільки ландшафт кібербезпеки змінюється.

He et al. розробили нове завдання безпеки, яке називається контрольованим генеруванням коду, для якого вони пропонують SVEN - підхід на основі навчання з додатковим бінарним параметром, доданим поряд із підказкою. Через цей бінарний параметр He et al. можуть визначити процес зміцнення безпеки або тестування на стійкість. Подібно до Pearce et al. [25], SVEN оцінюється, використовуючи підмножину Топ-25 CWE MITRE. He et al. виявили, що SVEN досягає високого рівня контролю безпеки через їхнє новаторське завдання.

Hajipour et al. [14] розглядають виклик знаходження вразливостей безпеки в LLM Codex та CodeGen, використовуючи підхід інверсії з малою кількістю прикладів. У цьому підході моделі ШІ підказуються, використовуючи приклади вразливого коду та підтримуючі підказки, щоб побачити, чи генерує вона вразливий код. Як і з усіма дослідженнями, обговорюваними тут, Hajipour et al. [14] використовують CodeQL для оцінки безпеки згенерованого коду. В результаті їхнього дослідження вони показали, що їхній підхід здатний знайти тисячі вразливостей безпеки в моделях ШІ, але потрібно більше роботи для визначення кращої методології для ідентифікації підказок, які призводять до вразливостей безпеки [14].

Tony et al. [31] фактично використали фрагменти вразливого коду на свою користь, щоб допомогти оцінити здатність GPT-3 та Code LLM генерувати безпечний код. Вони представили LLMSecEval - набір даних із 150 підказок природною мовою з описами фрагментів коду, схильних до вразливостей, які можна побачити в Топ-25 найпоширеніших перерахуваннях слабкостей (CWE) MITRE. Кожна підказка з набору даних мала пов'язану з нею безпечну реалізацію для порівняння відповідей GPT-3 та Codex. Подібний підхід можна використати для оцінки наявності принципів проектування безпеки в коді, згенерованому ШІ.

## 3. КЕЙС-СТАДІ: БЕЗПЕЧНЕ ЗБЕРІГАННЯ ПАРОЛІВ З CHATGPT

Ми представляємо кейс-стаді безпечного зберігання пароля - добре відоме завдання безпеки, яке було широко вивчене академічною дослідницькою спільнотою [15, 16, 21]. Однак література показує, що розробники мають проблеми з технічними аспектами безпечного зберігання паролів [15, 16, 21]. Крім того, у літературі існують добре встановлені критерії від Naiakshina et al. [21] для оцінки безпеки такого зберігання паролів. Отже, це служить корисним кейс-стаді для аналізу безпеки відповідей ChatGPT.

### 3.1 ChatGPT без явних вимог безпеки

Коли простим запитом попросили безпечно зберегти пароль у 5 вікнах підказок, ChatGPT запропонував реалізації, які відрізняються використаними бібліотеками: bcrypt, getpass та hashlib. Ми оцінили рівень безпеки реалізацій відповідно до критеріїв Naiakshina для безпечного зберігання пароля [21].

**Критерії оцінки безпечного зберігання паролів від Naiakshina [21]:**

• Пароль кінцевого користувача засолений (+1) та хешований (+1).
• Похідна довжина хешу становить принаймні 160 біт (+1).
• Кількість ітерацій для розтягування ключа становить принаймні 1,000 (+0.5) або 10,000 (+1) для PBKDF2 та принаймні 2¹⁰ для bcrypt (+1).
• Використовується функція хешування, важка для пам'яті (+1).
• Значення солі генерується випадково (+1).
• Сіль має довжину принаймні 32 біти (+1).

*Рисунок 1: Критерії оцінки зберігання паролів кінцевого користувача від Naiakshina [21], скопійовано дослівно. Оцінка ≥ 6 вказує на найкращі галузеві практики.*

Реалізації набрали в середньому 3 з 8 балів відповідно до критеріїв Naiakshina [21]. Чотири з п'яти реалізацій використовували бібліотеку bcrypt для завдання, після чого слідувало загальне використання функцій hashpw() та gensalt() для хешування та засолення пароля. Це принесло реалізації перші два доступні бали з критеріїв. Третій бал прийшов від того, що сіль мала довжину 32 біти. Такі кроки, як розтягування ключа, використання функцій хешування, важких для пам'яті, та довжина хешу принаймні 160 біт, не були розглянуті у відповідях ChatGPT.

### 3.2 ChatGPT з явними вимогами безпеки

Ми попросили ChatGPT написати код, який безпечно зберігає пароль, використовуючи критерії Naiakshina як список вимог безпеки. П'ять згенерованих реалізацій набрали в середньому 5 з 8 балів, з однією реалізацією, що набрала 8 з 8 балів. Ці п'ять реалізацій впровадили криптографічні алгоритми та функції, такі як PBKDF2, розтягування ключа та використання інших алгоритмів хешування, таких як SHA-256. Ми не бачили рекомендованих функцій хешування, важких для пам'яті, таких як scrypt та Argon2d.

У цьому короткому тесті ми бачимо, що визначаючи критерії безпеки, ChatGPT пропонує відповіді, які є більш безпечними.

Стандарт безпечного кодування C містить сотні прикладів невідповідного коду, який служив готовим набором даних помилок кодування. Sherman [18] пропустив їх через ChatGPT 3.5 та оцінив відповіді проти наданого виправленого коду. Коли справа доходить до аналізу коду, ChatGPT правильно ідентифікував проблему 46,2% часу. Це означало, що 52,1% часу ChatGPT не ідентифікував помилку кодування. Решта 1,7% припадає на естетичні проблеми, а не на помилку.

ChatGPT показав перспективи, але обмеження були очевидними [18]. Sherman зазначив, що основний механізм, який використовується LLM, залежить від співставлення шаблонів на основі навчальних даних. Ми виявили, що недоліки навичок аналізу програм ChatGPT можна пом'якшити інженерією підказок. Точно налаштовуючи підказки та визначаючи критерії безпеки, ChatGPT генерує більш безпечний код.

Розвиваючи модель водій/навігатор від Bird et al., ми можемо сказати, що хоча впровадження інструментів генерування коду на основі ШІ, таких як ChatGPT, заохочує зміщення ролі розробників від водія (автора коду) до навігатора (дизайнера завдань), заохочуване зміщення може бути передчасним [2].

Добре документовано, що розробники у своїй поточній ролі водія вважають виклик написання безпечного коду самостійно складним [1, 8, 20]. Виклики можна віднести до проблем зручності використання, що оточують API безпеки, та просто відсутності експертизи з безпеки для навігації цими API.

Важливо зазначити тут, що розробники програмного забезпечення взяли б початкові реалізації, написані без критеріїв безпеки Naiakshina, за номінальну вартість та вважали б їх безпечними. Це робить їх не тільки непридатними як водія, але й не підготованими до взяття на себе ролі навігатора. Роль навігатора вимагає від розробника проектувати завдання та вимоги безпеки для його досягнення. Без принципів безпеки для допомоги розробникам оцінити безпеку коду, згенерованого ШІ, додатки розробників відкриті для експлуатації та вразливостей [29, 31].

### 3.3 ChatGPT та Saltzer & Schroeder

Хоча модель GPT від OpenAI не дотримується принципів проектування Saltzer & Schroeder за своєю суттю, вона здатна запам'ятовувати контекст розмови. Це означало, що коли ми підняли принципи проектування Saltzer & Schroeder, модель намагалася максимально дотримуватися їхніх принципів, коли її просили згенерувати код. ChatGPT також пояснював у кожній відповіді, як згенерований ним код або дотримувався Saltzer & Schroeder, або ні.

Щоб краще зрозуміти процес, ми поділилися принципами, розглянутими у відповіді ChatGPT, та нашою оцінкою нижче:

• **Найменші привілеї**: ChatGPT: "Ми можемо побачити, що код обмежує свій доступ лише до необхідної функціональності, потрібної для досягнення завдання". Однак, успішно дотримуючись принципу найменших привілеїв, ChatGPT показав параметр saltCount функції getSalt() як непотрібний, коли насправді він необхідний для зміцнення зберігання пароля [21].

• **Безпечні за замовчуванням налаштування**: Як ми зазначили в розділі 3.1, 4 з 5 реалізацій від ChatGPT використовували бібліотеку bcrypt. Коли справа доходить до використання алгоритму хешування, bcrypt використовує алгоритм Blowfish як безпечне за замовчуванням налаштування, що є прийнятним, але доступні більш безпечні алгоритми.

• **Повне посередництво**: Як ми згадували раніше, принципи Saltzer & Schroeder вплинули на роботи інших через адаптації з часом. ChatGPT: "Всі функції в коді, згенерованому ChatGPT, опосередковуються через бібліотеку bcrypt. Розробникам не потрібно впроваджувати власні криптографічні функції або шукати в іншому місці, що зменшує ризик помилок реалізації."

---

## ПЕРЕКЛАД ВИКОНАНО

**Автори оригіналу**: Nikhil Patnaik, Joseph Hallett, Awais Rashid  
**Переклад українською**: Claude  
**Дата**: 2024  

### Примітки до перекладу:

1. **Security-by-design** перекладено як "безпека за замовчуванням" для кращого розуміння в українському контексті
2. **Large Language Models (LLMs)** залишено як "великі мовні моделі" з абревіатурою в дужках
3. **API** залишено без перекладу як загальноприйнятий термін
4. Назви продуктів (ChatGPT, GitHub Copilot, Multics) залишено в оригінальному написанні
5. Прізвища дослідників (Saltzer, Schroeder, Gutmann, Naiakshina) залишено в оригінальній транскрипції
6. **Code smells** перекладено як "запахи коду" - усталений термін в українській ІТ-спільноті
7. **CWE (Common Weakness Enumerations)** - залишено з поясненням українською
