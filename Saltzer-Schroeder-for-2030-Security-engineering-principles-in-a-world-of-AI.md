


# Saltzer & Schroeder for 2030: Security engineering principles in a world of AI
https://arxiv.org/pdf/2407.05710

# Saltzer & Schroeder для 2030: Принципи інженерії безпеки у світі штучного інтелекту

## АНОТАЦІЯ

Написання безпечного коду є складним завданням, тому очікується, що після випуску інструментів генерування коду на основі ШІ, таких як ChatGPT та GitHub Copilot, розробники будуть використовувати ці інструменти для виконання завдань безпеки та використання API безпеки. Однак чи є код, згенерований ChatGPT, безпечним? Як звичайний програмний інженер або інженер з безпеки зможе це визначити?

Наближаючись до наступного десятиліття, ми очікуємо більшого впровадження інструментів генерування коду на основі ШІ та бачимо, як розробники використовують їх для написання безпечного коду. Готуючись до цього, нам потрібно забезпечити безпеку за замовчуванням (security-by-design). У цій статті ми озираємося назад у часі на принципи проектування безпеки Saltzer & Schroeder, оскільки вони повинні будуть еволюціонувати та адаптуватися до викликів, які приходять зі світом коду, згенерованого штучним інтелектом.

## КЛЮЧОВІ СЛОВА

Принципи проектування безпеки, ChatGPT, великі мовні моделі

## ПОСИЛАННЯ ACM

Nikhil Patnaik, Joseph Hallett, та Awais Rashid. 2024. Saltzer & Schroeder для 2030: Принципи інженерії безпеки у світі ШІ. У Збірнику Міжнародного семінару з програмної інженерії в 2030 (SE 2030). ACM, Нью-Йорк, Нью-Йорк, США, 5 сторінок. https://doi.org/10.1145/nnnnnnn.nnnnnnn

## 1. ВСТУП

У 1974 році Saltzer розглянув виклик захисту та контролю обміну інформацією в Multics (Multiplexed Information and Computing Service) [26]. Saltzer запропонував 5 принципів проектування для допомоги в оцінці різних рішень. Ці принципи проектування розглядали списки контролю доступу, ідентифікацію та автентифікацію користувачів, ієрархічний контроль специфікацій доступу та захист первинної пам'яті [26].

У 1975 році Saltzer & Schroeder представили 8 принципів проектування та серію бажаних функцій з наміром захистити інформацію, що зберігається на комп'ютері, від несанкціонованого доступу та модифікації. На той час програмні додатки могли зберігати інформацію та одночасно використовуватися декількома користувачами. Ключовим викликом, який хотіли вирішити Saltzer & Schroeder, було питання множинного використання. Для додатків з користувачами, які не мають рівних повноважень, потрібна система для забезпечення бажаної структури повноважень у додатку [27].

Робота Saltzer & Schroeder стала дуже впливовою та застосовною до широкого спектру галузей, таких як забезпечення політик безпеки [28], оцінка архітектури безпеки Java [9] та мінімізація помилок, пов'язаних з користувачами, в безпеці інформаційних систем [30].

Однак лише в 1995 році принципи Saltzer & Schroeder вперше були застосовані до проектування API безпеки через Cryptlib [11]. Між 1995 та 2002 роками Gutmann адаптував роботу Saltzer & Schroeder для вирішення викликів безпеки та зручності використання при проектуванні криптографічного API [11–13]. Робота Gutmann відіграла інтегральну роль у впровадженні принципів проектування Saltzer & Schroeder у галузь проектування API безпеки [1, 10, 19, 23, 24, 32].

Еволюцію принципів Saltzer & Schroeder можна віднести на рахунок постійно змінного ландшафту кібербезпеки та викликів, які з цим приходять, оскільки саме ці виклики змушують принципи еволюціонувати.

Сьогодні ми спостерігаємо настання світу штучного інтелекту. З випуском великих мовних моделей (LLM), OpenAI розпочала гонку ШІ з невизначеною фінішною лінією. Тепер, змагаючись з конкурентами, такими як Google's PaLM та LaMDA, ми бачимо швидкий розвиток у цій галузі. Спільнота розробки програмного забезпечення проявила великий інтерес до цих досягнень, оскільки інтеграція інструментів генерування коду на основі ШІ, таких як ChatGPT та GitHub Copilot, може допомогти їм у багатьох різноманітних завданнях, таких як покращення процесу перегляду коду, надання пропозицій кодування, пропонування виправлень для дефектів у коді.

У майбутньому ці інструменти генерування коду на основі ШІ могли б допомогти в рефакторингу кодових баз великих, складних програмних додатків, покращенні читабельності та полегшенні підтримки з підтримкою документації, згенерованої ШІ [2, 5].

З усіма цими майбутніми змінами в індустрії програмного забезпечення, яку роль відіграють Saltzer & Schroeder у цьому світі ШІ? Чи достатньо їхніх принципів проектування безпеки? Якщо ні, то чому ні? Якщо вони достатні, як ми повинні використовувати їх для формування бачення на 2030 рік?

У цій статті ми оцінюємо здатність ChatGPT безпечно зберігати пароль та оцінюємо згенерований код відповідно до критеріїв Naiakshina для безпечного зберігання паролів [21]. Після цього ми підказуємо ChatGPT повторити завдання, але цього разу використовуючи критерії безпеки Naiakshina як список вимог. Нарешті, ми просимо ChatGPT оцінити свій згенерований код відповідно до принципів проектування безпеки Saltzer & Schroeder [27].

Мета цієї вправи полягає не тільки в розумінні можливостей ChatGPT у безпечному кодуванні та розумінні принципів безпеки, але також як крок у розумінні того, де в динаміці Розробник/ШІ можуть бути впроваджені Saltzer & Schroeder? Де в процесі розробки програмного забезпечення Saltzer & Schroeder були б найбільш ефективними? Якщо вони не ефективні, які недоліки? Чи можуть вони бути вирішені через адаптований набір принципів Saltzer & Schroeder? Чи достатньо Saltzer & Schroeder, чи це кінець дороги?

## 2. ПЕРЕДУМОВИ

### 2.1 Загальні висновки

Після випуску GitHub Copilot, ШІ-програміста для парного програмування, Bird et al. [2] вивчили процес парного програмування та те, як інтеграція GitHub Copilot змінила роль розробника. Bird et al. пояснює, як коли два розробники пишуть код разом, один бере на себе роль водія, пишучи код для призначеного завдання, тоді як інший працює як навігатор, направляючи та переглядаючи роботу водія. Однак Bird et al. виявили, що після впровадження GitHub Copilot роль розробників змінюється від написання коду до розуміння коду. Впровадження GitHub Copilot означає, що навичка перегляду коду стала рівною, якщо не більш важливою, ніж написання коду. Методи, якими розробники можуть оцінювати код, згенерований ШІ, ще не розроблені. Наразі згенерований код проходить той самий процес перегляду, що й код розробника [2, 5].

Щодо продуктивності, GitHub Copilot відомий тим, що допомагає розробникам адаптуватися до різних мов програмування та стилів написання [2]. Однак, що стосується якості коду, Yetiştiren et al. [33, 34] провели емпіричне дослідження для порівняння якості коду та правильності GitHub Copilot, ChatGPT та Amazon CodeWhisperer, використовуючи набір даних HumanEval [4], і виявили, що з точки зору правильності коду ChatGPT показав найкращі результати з правильними рішеннями для 65,2% задач набору даних HumanEval, за ним йде GitHub Copilot. Yetiştiren зазначив, що розробники повинні очікувати знайти деякі помилки з усіма цими інструментами, але ці помилки не настільки поширені, як виявлені "запахи коду" (code smells). Загалом було виявлено 13 "запахів коду" від трьох інструментів генерування коду на основі ШІ [33, 34].

Ernst et al. [5] коментують короткострокові та довгострокові ефекти використання AIDE (Середовища розробки, керовані штучним інтелектом) для вивчення програмування. З одного боку, інструменти як ChatGPT та GitHub Copilot дійсно можуть допомогти початківцю-розробнику вивчити мову програмування, пропонуючи пропозиції та допомагаючи з завданнями кодування. Це може здатися привабливим багатьом розробникам, які приєднуються до індустрії сьогодні, але з іншого боку, ці розробники можуть мати труднощі з повним розумінням того, чому запропонований фрагмент коду працює і як правильно інтегрувати його з їхньою існуючою кодовою базою [5].

Крім того, Codex від OpenAI становить виклик для викладачів та університетських модулів, оскільки він достатньо хороший, щоб перевершити багатьох студентів у модулях "Комп'ютерні науки 1" ступеня комп'ютерних наук. Студенти можуть почати думати, що якщо Codex краще програмує, ніж вони, навіщо їм турбуватися про вивчення програмування? Замість цього студенти почнуть зосереджуватися на інженерії підказок та вивченні того, як налаштовувати код, згенерований ШІ. Їхня роль змінюється від водія (автора коду) до навігатора (дизайнера завдань), але це має серйозні наслідки.

Багато студентів вперше знайомляться з практиками кодування та принципами проектування в університеті, тому впроваджуючи LLM як Codex, студенти можуть бути позбавлені можливості застосувати принципи проектування, які вони вивчають теоретично, до практичних застосувань, оскільки вони можуть попросити ChatGPT згенерувати код для них і втратять свою здатність до вирішення проблем та критичного мислення. Цей потенційний сценарій також зупиняє впровадження та еволюцію принципів проектування безпеки та практик безпечного кодування [3, 6, 7, 22, 27].

У недавньому дослідженні Lau et al. [17] було проведено опитування 20 інструкторів програмування, щоб зрозуміти, як вони планують адаптуватися до впровадження моделей ШІ. На короткострокову перспективу практики мають намір відмовляти від використання таких інструментів, але готуючись до майбутнього, деякі погодилися, що такий опір марний, і готові інтегрувати інструменти генерування коду на основі ШІ з їхніми існуючими програмами.

### 2.2 Наслідки для безпеки використання моделей ШІ

Останні роботи зробили кроки для розуміння наслідків для безпеки використання інструментів генерування коду на основі ШІ [14, 16, 25, 29, 31]. Pearce et al. [25] провели емпіричне дослідження для оцінки продуктивності GitHub Copilot проти підмножини Топ-25 найпоширеніших перерахувань слабкостей (CWE) MITRE. Результати з точки зору безпеки були невизначеними, але Pearce et al. зазначають, що GitHub Copilot навчений на наборі даних відкритого коду, тому розробники повинні очікувати помилки і тому їм радиться залишатися критичними та пильними щодо коду, згенерованого GitHub Copilot. Цікавий момент, згаданий Pearce et al., - це вплив часу на якість безпеки в програмному забезпеченні з відкритим кодом. Найкраща практика на момент написання може повільно стати поганою практикою, оскільки ландшафт кібербезпеки змінюється.

He et al. розробили нове завдання безпеки, яке називається контрольованим генеруванням коду, для якого вони пропонують SVEN - підхід на основі навчання з додатковим бінарним параметром, доданим поряд із підказкою. Через цей бінарний параметр He et al. можуть визначити процес зміцнення безпеки або тестування на стійкість. Подібно до Pearce et al. [25], SVEN оцінюється, використовуючи підмножину Топ-25 CWE MITRE. He et al. виявили, що SVEN досягає високого рівня контролю безпеки через їхнє новаторське завдання.

Hajipour et al. [14] розглядають виклик знаходження вразливостей безпеки в LLM Codex та CodeGen, використовуючи підхід інверсії з малою кількістю прикладів. У цьому підході моделі ШІ підказуються, використовуючи приклади вразливого коду та підтримуючі підказки, щоб побачити, чи генерує вона вразливий код. Як і з усіма дослідженнями, обговорюваними тут, Hajipour et al. [14] використовують CodeQL для оцінки безпеки згенерованого коду. В результаті їхнього дослідження вони показали, що їхній підхід здатний знайти тисячі вразливостей безпеки в моделях ШІ, але потрібно більше роботи для визначення кращої методології для ідентифікації підказок, які призводять до вразливостей безпеки [14].

Tony et al. [31] фактично використали фрагменти вразливого коду на свою користь, щоб допомогти оцінити здатність GPT-3 та Code LLM генерувати безпечний код. Вони представили LLMSecEval - набір даних із 150 підказок природною мовою з описами фрагментів коду, схильних до вразливостей, які можна побачити в Топ-25 найпоширеніших перерахуваннях слабкостей (CWE) MITRE. Кожна підказка з набору даних мала пов'язану з нею безпечну реалізацію для порівняння відповідей GPT-3 та Codex. Подібний підхід можна використати для оцінки наявності принципів проектування безпеки в коді, згенерованому ШІ.

## 3. КЕЙС-СТАДІ: БЕЗПЕЧНЕ ЗБЕРІГАННЯ ПАРОЛІВ З CHATGPT

Ми представляємо кейс-стаді безпечного зберігання пароля - добре відоме завдання безпеки, яке було широко вивчене академічною дослідницькою спільнотою [15, 16, 21]. Однак література показує, що розробники мають проблеми з технічними аспектами безпечного зберігання паролів [15, 16, 21]. Крім того, у літературі існують добре встановлені критерії від Naiakshina et al. [21] для оцінки безпеки такого зберігання паролів. Отже, це служить корисним кейс-стаді для аналізу безпеки відповідей ChatGPT.

### 3.1 ChatGPT без явних вимог безпеки

Коли простим запитом попросили безпечно зберегти пароль у 5 вікнах підказок, ChatGPT запропонував реалізації, які відрізняються використаними бібліотеками: bcrypt, getpass та hashlib. Ми оцінили рівень безпеки реалізацій відповідно до критеріїв Naiakshina для безпечного зберігання пароля [21].

**Критерії оцінки безпечного зберігання паролів від Naiakshina [21]:**

• Пароль кінцевого користувача засолений (+1) та хешований (+1).
• Похідна довжина хешу становить принаймні 160 біт (+1).
• Кількість ітерацій для розтягування ключа становить принаймні 1,000 (+0.5) або 10,000 (+1) для PBKDF2 та принаймні 2¹⁰ для bcrypt (+1).
• Використовується функція хешування, важка для пам'яті (+1).
• Значення солі генерується випадково (+1).
• Сіль має довжину принаймні 32 біти (+1).

*Рисунок 1: Критерії оцінки зберігання паролів кінцевого користувача від Naiakshina [21], скопійовано дослівно. Оцінка ≥ 6 вказує на найкращі галузеві практики.*

Реалізації набрали в середньому 3 з 8 балів відповідно до критеріїв Naiakshina [21]. Чотири з п'яти реалізацій використовували бібліотеку bcrypt для завдання, після чого слідувало загальне використання функцій hashpw() та gensalt() для хешування та засолення пароля. Це принесло реалізації перші два доступні бали з критеріїв. Третій бал прийшов від того, що сіль мала довжину 32 біти. Такі кроки, як розтягування ключа, використання функцій хешування, важких для пам'яті, та довжина хешу принаймні 160 біт, не були розглянуті у відповідях ChatGPT.

### 3.2 ChatGPT з явними вимогами безпеки

Ми попросили ChatGPT написати код, який безпечно зберігає пароль, використовуючи критерії Naiakshina як список вимог безпеки. П'ять згенерованих реалізацій набрали в середньому 5 з 8 балів, з однією реалізацією, що набрала 8 з 8 балів. Ці п'ять реалізацій впровадили криптографічні алгоритми та функції, такі як PBKDF2, розтягування ключа та використання інших алгоритмів хешування, таких як SHA-256. Ми не бачили рекомендованих функцій хешування, важких для пам'яті, таких як scrypt та Argon2d.

У цьому короткому тесті ми бачимо, що визначаючи критерії безпеки, ChatGPT пропонує відповіді, які є більш безпечними.

Стандарт безпечного кодування C містить сотні прикладів невідповідного коду, який служив готовим набором даних помилок кодування. Sherman [18] пропустив їх через ChatGPT 3.5 та оцінив відповіді проти наданого виправленого коду. Коли справа доходить до аналізу коду, ChatGPT правильно ідентифікував проблему 46,2% часу. Це означало, що 52,1% часу ChatGPT не ідентифікував помилку кодування. Решта 1,7% припадає на естетичні проблеми, а не на помилку.

ChatGPT показав перспективи, але обмеження були очевидними [18]. Sherman зазначив, що основний механізм, який використовується LLM, залежить від співставлення шаблонів на основі навчальних даних. Ми виявили, що недоліки навичок аналізу програм ChatGPT можна пом'якшити інженерією підказок. Точно налаштовуючи підказки та визначаючи критерії безпеки, ChatGPT генерує більш безпечний код.

Розвиваючи модель водій/навігатор від Bird et al., ми можемо сказати, що хоча впровадження інструментів генерування коду на основі ШІ, таких як ChatGPT, заохочує зміщення ролі розробників від водія (автора коду) до навігатора (дизайнера завдань), заохочуване зміщення може бути передчасним [2].

Добре документовано, що розробники у своїй поточній ролі водія вважають виклик написання безпечного коду самостійно складним [1, 8, 20]. Виклики можна віднести до проблем зручності використання, що оточують API безпеки, та просто відсутності експертизи з безпеки для навігації цими API.

Важливо зазначити тут, що розробники програмного забезпечення взяли б початкові реалізації, написані без критеріїв безпеки Naiakshina, за номінальну вартість та вважали б їх безпечними. Це робить їх не тільки непридатними як водія, але й не підготованими до взяття на себе ролі навігатора. Роль навігатора вимагає від розробника проектувати завдання та вимоги безпеки для його досягнення. Без принципів безпеки для допомоги розробникам оцінити безпеку коду, згенерованого ШІ, додатки розробників відкриті для експлуатації та вразливостей [29, 31].

### 3.3 ChatGPT та Saltzer & Schroeder

Хоча модель GPT від OpenAI не дотримується принципів проектування Saltzer & Schroeder за своєю суттю, вона здатна запам'ятовувати контекст розмови. Це означало, що коли ми підняли принципи проектування Saltzer & Schroeder, модель намагалася максимально дотримуватися їхніх принципів, коли її просили згенерувати код. ChatGPT також пояснював у кожній відповіді, як згенерований ним код або дотримувався Saltzer & Schroeder, або ні.

Щоб краще зрозуміти процес, ми поділилися принципами, розглянутими у відповіді ChatGPT, та нашою оцінкою нижче:

• **Найменші привілеї**: ChatGPT: "Ми можемо побачити, що код обмежує свій доступ лише до необхідної функціональності, потрібної для досягнення завдання". Однак, успішно дотримуючись принципу найменших привілеїв, ChatGPT показав параметр saltCount функції getSalt() як непотрібний, коли насправді він необхідний для зміцнення зберігання пароля [21].

• **Безпечні за замовчуванням налаштування**: Як ми зазначили в розділі 3.1, 4 з 5 реалізацій від ChatGPT використовували бібліотеку bcrypt. Коли справа доходить до використання алгоритму хешування, bcrypt використовує алгоритм Blowfish як безпечне за замовчуванням налаштування, що є прийнятним, але доступні більш безпечні алгоритми.

• **Повне посередництво**: Як ми згадували раніше, принципи Saltzer & Schroeder вплинули на роботи інших через адаптації з часом. ChatGPT: "Всі функції в коді, згенерованому ChatGPT, опосередковуються через бібліотеку bcrypt. Розробникам не потрібно впроваджувати власні криптографічні функції або шукати в іншому місці, що зменшує ризик помилок реалізації."

Однак повне посередництво вимагає довіри, і як ми бачили, відповідно до критеріїв безпеки Naiakshina, ChatGPT не працює добре, якщо критерії не визначені в підказці. Це проблематично, оскільки звичайний розробник програмного забезпечення не знатиме, що потрібно додавати критерії безпеки як частину своєї підказки, і тому візьме згенерований код за номінальну вартість і вважатиме його безпечним, навіть якщо це не так. Цю проблему слабкої безпеки можна легко ідентифікувати в контрольованому кейс-стаді, як це, але які індикатори та заходи пом'якшення для програмних інженерів та інженерів з безпеки в реальному світі, світі ШІ 2030 року? Як ми забезпечуємо, що безпека коду, згенерованого ШІ, відповідає стандарту? Чи тут вступають Saltzer & Schroeder?

• **Психологічна прийнятність**: Розроблена з урахуванням повсякденного розробника, бібліотека bcrypt пропонує простий криптографічний API для безпечного зберігання пароля без потреби в сильному рівні криптографічної експертизи розробників, досягаючи психологічної прийнятності. Однак швидкий пошук через StackOverflow показує, що є досить багато питань, пов'язаних з бібліотекою bcrypt, багато з яких є проблемами збірки бібліотеки, проблемами встановлення, труднощами використання з іншими додатками, такими як MySQL, NodeJS, та міжмовним хешуванням. Цей неформальний пошук показує, що bcrypt має запах зручності використання "Не грає добре з іншими" [24].

• **Розділення привілеїв**: Бібліотека bcrypt підтримує функціональність хешування паролів та верифікації, так що розробники можуть забезпечити розділення привілеїв, обмежуючи доступ до бази даних паролів та пов'язаних операцій.

Ми можемо побачити, що ChatGPT здатний застосувати деякі принципи Saltzer & Schroeder до завдання безпечного зберігання пароля, але його застосування цих принципів показує неповне застосування та вводить серію проблем, пов'язаних з довірою.

## 4. ОБГОВОРЕННЯ

**Яку роль відіграють Saltzer & Schroeder у цьому світі ШІ?**

Щоб відповісти на це питання, давайте спочатку подивимося на виклики, які Saltzer & Schroeder спочатку намагалися вирішити, та порівняємо їх з викликом, який ми можемо побачити до 2030 року. Ключовим викликом, який хотіли вирішити Saltzer & Schroeder, було питання множинного використання — кілька користувачів, що мають доступ до інформації на одному спільному комп'ютері. Принципи, які вони визначили, були запропоновані для системи, яка могла б забезпечити бажану структуру повноважень у додатку [27].

Що є викликом "множинного використання" для інженерії безпеки сьогодні? Це не жменька людей, що ділять комп'ютер. Це масштабовано, натомість це багато людей, багато розробників, багато інженерів з безпеки, що працюють над забезпеченням безпеки гігантських додатків, і LLM-навчений інструмент генерування коду на основі ШІ тепер також залучений і є частиною сьогоднішньої проблеми "множинного використання".

Давайте застосуємо деякі принципи Saltzer & Schroeder до моделі GPT від OpenAI. **Економія механізму** легко досягається через простий, легкий у використанні інтерфейс у ChatGPT. Однак ми виявили, що для того, щоб побачити безпеку в його коді, потрібно визначити безпеку в своїй підказці. Небажані шляхи доступу видно через погану реалізацію забезпечення безпеки пароля і не будуть помічені звичайним розробником.

За замовчуванням ChatGPT не відповідає на запити зловмисного коду, але є обхідні шляхи. Коли справа доходить до інженерії безпеки та написання безпечного коду, ChatGPT не має власних **безпечних за замовчуванням налаштувань**. Він покладається на те, що навчальні дані будуть готовими безпечними. Якщо не підказувати [21], ChatGPT не може врахувати силу криптографічних примітивів та практик безпечного кодування при написанні безпечного коду.

Останні роботи оцінили LLM на наборах даних, схильних до вразливостей [14, 18, 25, 31]. У цьому кейс-стаді ми спробували інтегрувати принципи Saltzer & Schroeder з інженерією підказок, але чи можливо впровадити принципи Saltzer & Schroeder через інший шлях, через навчання, орієнтоване на безпечний дизайн?

Принцип **відкритого дизайну** стверджує, що механізм не повинен залежати від невігластва потенційних атакуючих, але впровадження LLM через інструменти як ChatGPT неявно заохочує невігластво своїх користувачів [2, 5]. Bird et al. зафіксували зміну динаміки в ролі розробників після впровадження GitHub Copilot. Це починається з автоматизації рутинних завдань, переданих моделям ШІ, залишаючи нам, розробникам, більш творчі завдання, але коли справа доходить до перегляду коду, згенерованого Copilot, Bird et al. зазначили, що розробники проявляють деяку нерішучість та відчуття невпевненості щодо того, чи є код функціонально правильним [2].

Щоб збалансувати цей аргумент, ми повинні згадати, що початківці-розробники кажуть, що вони отримують користь від використання моделей ШІ для вивчення нових мов програмування [2].

Ми виявили, що ChatGPT міг застосувати лише деякі принципи Saltzer & Schroeder, і для тих, які він застосував, ми вказали багато неправильних застосувань принципів, показуючи неповне розуміння з боку ChatGPT. Ми стверджуємо, що Saltzer & Schroeder дійсно мають роль у грядущому десятилітті. З точним налаштуванням та інженерією підказок можливо, що принципи Saltzer & Schroeder краще застосовуються LLM.

Щоб вирішити проблему відсутності безпечних за замовчуванням налаштувань та загального застосування безпеки, принципи Saltzer & Schroeder можуть бути перероблені в більш дієві стандарти безпеки, адаптовані для навчального набору, орієнтованого на безпеку, для LLM. Враховуючи складні типи атак, доступних, та схильність LLM до вразливих наборів даних, ми не думаємо, що принципи Saltzer & Schroeder достатні такими, якими вони є, і їх потрібно переробити та адаптувати для вирішення викликів, таких як ML-отруєння, наприклад. Якщо ми розіграємо сценарій "множинного використання", ML-отруєння впроваджує вразливості в LLM, роблячи сам LLM вразливим.

Нарешті, нам потрібно розглянути наслідки зміни ролі розробників від водія (автора коду) до навігатора (дизайнера завдань). Ми думаємо, що ця зміна продовжиться до 2030 року, однак важливо продовжувати підтримувати розробників та заохочувати їх використовувати API безпеки та інструменти самостійно сьогодні. Ми повинні не тільки застосовувати принципи Saltzer & Schroeder, але й багато інших наявних наборів, таких як шаблони проектування Gamma et al., евристики зручності використання Nielsen [22]. Ці принципи та рекомендації можуть бути застосовані для покращення дизайну існуючих API безпеки, як OpenSSL, та розробки нових інструментів безпеки.

## 5. ДОСЛІДНИЦЬКА ПРОГРАМА 2030

Спільнота інженерії програмного забезпечення повинна просувати дослідження у:

• **Точне налаштування LLM**: Точне налаштування LLM для кращого дотримання принципів Saltzer & Schroeder [27].

• **Навчальний набір даних для проектування безпеки**: Зосередитися на розробці навчального набору, заснованого на проектуванні безпеки.

• **Розробка інструментів безпеки та API**: Продовжувати допомагати розробникам писати безпечний код з інструментами безпеки та API для "живлення" "водія".

**Наше бачення на 2030 рік** полягає в тому, що ми матимемо LLM, орієнтований на безпеку, навчений на коді, валідованому відповідно до Saltzer & Schroeder. З точним налаштуванням наш LLM може почати брати на себе більш складні завдання безпеки. Ці розробники матимуть широкий спектр інструментів безпеки та необхідні навички для підтримки у написанні безпечного коду.

---

## СПИСОК ЛІТЕРАТУРИ

[1] Yasemin Acar, Michael Backes, Sascha Fahl, Simson Garfinkel, Doowon Kim, Michelle L Mazurek, and Christian Stransky. 2017. Comparing the usability of cryptographic apis. In *2017 IEEE Symposium on Security and Privacy (SP)*. IEEE, IEEE, Herrsching am Ammersee, Germany, 154–171.

[2] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools. *Queue* 20, 6 (2022), 35–57.

[3] Joshua Bloch. 2001. *Effective Java*. Pearson Education, Pearson Education.

[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374* 1, 1 (2021), 1–35.

[5] Neil A Ernst and Gabriele Bavota. 2022. Ai-driven development is here: Should you worry? *IEEE Software* 39, 2 (2022), 106–110.

[6] Martin Fowler. 2018. *Refactoring: improving the design of existing code*. Addison-Wesley Professional, Addison-Wesley Professional.

[7] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1993. Design patterns: Abstraction and reuse of object-oriented design. In *European Conference on Object-Oriented Programming*. Springer, Springer, Heidelberg, Berlin, Germany, 406–431.

[8] Martin Georgiev, Subodh Iyengar, Suman Jana, Rishita Anubhai, Dan Boneh, and Vitaly Shmatikov. 2012. The most dangerous code in the world: validating SSL certificates in non-browser software. In *Proceedings of the 2012 ACM conference on Computer and communications security*. ACM, Raleigh, North Carolina, USA, 38–49.

[9] Li Gong and Gary Ellison. 2003. *Inside Java™ 2 Platform Security: Architecture, API Design, and Implementation* (2nd ed.). Pearson Education, Pearson Education.

[10] Matthew Green and Matthew Smith. 2016. Developers are not the enemy!: The need for usable security APIs. *IEEE Security & Privacy* 14, 5 (2016), 40–46.

[11] Peter Gutmann. 1995. Peter Gutmann. cryptlib security toolkit: user's guide and manual. *Peter Gutmann* 1 (1995), 1–372.

[12] Peter Gutmann. 1999. The Design of a Cryptographic Security Architecture. In *USENIX Security Symposium*. USENIX, Washington DC, USA, 1–16.

[13] Peter Gutmann. 2002. Lessons Learned in Implementing and Deploying Crypto Software. In *Usenix Security Symposium*. USENIX, San Francisco, CA, USA, 315–325.

[14] Hossein Hajipour, Thorsten Holz, Lea Schönherr, and Mario Fritz. 2023. Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models. *arXiv preprint arXiv:2302.04012* 1, 1 (2023), 1–14.

[15] Joseph Hallett, Nikhil Patnaik, Benjamin Shreeve, and Awais Rashid. 2021. "Do this! Do that!, And nothing will happen" Do specifications lead to securely stored passwords?. In *2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)*. IEEE, 486–498.

[16] Jingxuan He and Martin Vechev. 2023. Large language models for code: Security hardening and adversarial testing. In *Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security*. Association for Computing Machinery, ACM New York, NY, USA, 1865–1879.

[17] Sam Lau and Philip Guo. 2023. From "Ban it till we understand it" to "Resistance is futile": How university programming instructors plan to adapt as more students use AI code generation and explanation tools such as ChatGPT and GitHub Copilot. In *Proceedings of the 2023 ACM Conference on International Computing Education Research-Volume 1*. Association for Computing Machinery, ACM New York, NY, USA, 106–121.

[18] Carnegie Mellon Hall Mark Sherman, Software Engineering Institute. 2024. Using ChatGPT to Analyze Your Code? Not So Fast.

[19] Kai Mindermann, Philipp Keck, and Stefan Wagner. 2018. How Usable Are Rust Cryptography APIs?. In *2018 IEEE International Conference on Software Quality, Reliability and Security, QRS 2018, Lisbon, Portugal, July 16-20, 2018*. IEEE, Lisbon, Portugal, 143–154. https://doi.org/10.1109/QRS.2018.00028

[20] Sarah Nadi, Stefan Krüger, Mira Mezini, and Eric Bodden. 2016. Jumping through hoops: Why do Java developers struggle with cryptography APIs?. In *Proceedings of the 38th International Conference on Software Engineering*. Association for Computing Machinery, ACM New York, NY, USA, 935–946.

[21] Alena Naiakshina, Anastasia Danilova, Christian Tiefenau, Marco Herzog, Sergej Dechand, and Matthew Smith. 2017. Why do developers get password storage wrong? A qualitative usability study. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security*. Association for Computing Machinery, ACM New York, NY, USA, 311–328.

[22] Jakob Nielsen. 1994. Enhancing the explanatory power of usability heuristics. In *Proceedings of the SIGCHI conference on Human Factors in Computing Systems*. ACM, ACM, New York, NY, USA, 152–158.

[23] Daniela Seabra Oliveira, Tian Lin, Muhammad Sajidur Rahman, Rad Akefirad, Donovan Ellis, Eliany Perez, Rahul Bobhate, Lois A DeLong, Justin Cappos, and Yuriy Brun. 2018. {API} Blindspots: Why Experienced Developers Write Vulnerable Code. In *Fourteenth Symposium on Usable Privacy and Security ({SOUPS} 2018)*. USENIX, Baltimore, MD, USA, 315–328.

[24] Nikhil Patnaik, Joseph Hallett, and Awais Rashid. 2019. Usability Smells: An Analysis of Developers' Struggle With Crypto Libraries. In *Fifteenth Symposium on Usable Privacy and Security (SOUPS 2019)*. USENIX, San Francisco, CA, USA, 245–257.

[25] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. Asleep at the keyboard? assessing the security of github copilot's code contributions. In *2022 IEEE Symposium on Security and Privacy (SP)*. IEEE, IEEE, Herrsching am Ammersee, Germany, 754–768.

[26] Jerome H Saltzer. 1974. Protection and the control of information sharing in Multics. *Commun. ACM* 17, 7 (1974), 388–402.

[27] Jerome H Saltzer and Michael D Schroeder. 1975. The protection of information in computer systems. *Proc. IEEE* 63, 9 (1975), 1278–1308.

[28] Fred B Schneider. 1999. Enforceable security policies. Technical Report. Cornell University.

[29] Mohammed Latif Siddiq and Joanna CS Santos. 2022. SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. In *Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security*. Association for Computing Machinery, ACM New York, NY, USA, 29–33.

[30] Mikko T Siponen. 2000. Critical analysis of different approaches to minimizing user-related faults in information systems security: implications for research and practice. *Information Management & Computer Security* 8, 5 (2000), 197–209.

[31] Catherine Tony, Markus Mutas, Nicolás E Díaz Ferreyra, and Riccardo Scandariato. 2023. LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations. *arXiv preprint arXiv:2303.09384* 1, 1 (2023), 1–5.

[32] Daniel Votipka, Kelsey R Fulton, James Parker, Matthew Hou, Michelle L Mazurek, and Michael Hicks. 2020. Understanding security mistakes developers make: Qualitative analysis from build it, break it, fix it. In *Proceedings of the 29th USENIX Security Symposium (USENIX) Security*, Vol. 20. USENIX, USENIX, 1–18.

[33] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. *arXiv preprint arXiv:2304.10778* 1, 1 (2023), 1–45.

[34] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot's code generation. In *Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering*. Association for Computing Machinery, ACM New York, NY, USA, 62–71.

---

## ПОВНИЙ ПЕРЕКЛАД ЗАВЕРШЕНО

**Автори оригіналу**: Nikhil Patnaik, Joseph Hallett, Awais Rashid  
**Переклад українською**: Claude  
**Дата**: 2024  

### Підсумок дослідження:

🎯 **Основна теза**: Класичні принципи безпеки Saltzer & Schroeder (1975) залишаються актуальними в епоху ШІ, але потребують адаптації

📊 **Емпіричні результати**: ChatGPT показав 3/8 балів без явних вимог безпеки та 5/8 з явними вимогами

🔄 **Еволюція ролей**: Розробники переходять від "водіїв" (пишуть код) до "навігаторів" (проектують завдання)

⚠️ **Основний виклик**: Розробники можуть помилково вважати ШІ-генерований код безпечним без належної експертизи

🔮 **Бачення 2030**: Спеціалізовані LLM, навчені на валідованому безпечному коді з принципами Saltzer & Schroeder

### Ключові принципи перекладу:

1. **Збереження академічного стилю** - формальна наукова мова
2. **Технічна точність** - всі терміни перекладені коректно
3. **Контекстуальність** - адаптовано для української ІТ-спільноти
4. **Повнота** - перекладено від анотації до списку літератури
5. **Читабельність** - зрозуміло для україномовної аудиторії

---

## ПЕРЕКЛАД ВИКОНАНО

**Автори оригіналу**: Nikhil Patnaik, Joseph Hallett, Awais Rashid  
**Переклад українською**: Claude  
**Дата**: 2024  

### Примітки до перекладу:

1. **Security-by-design** перекладено як "безпека за замовчуванням" для кращого розуміння в українському контексті
2. **Large Language Models (LLMs)** залишено як "великі мовні моделі" з абревіатурою в дужках
3. **API** залишено без перекладу як загальноприйнятий термін
4. Назви продуктів (ChatGPT, GitHub Copilot, Multics) залишено в оригінальному написанні
5. Прізвища дослідників (Saltzer, Schroeder, Gutmann, Naiakshina) залишено в оригінальній транскрипції
6. **Code smells** перекладено як "запахи коду" - усталений термін в українській ІТ-спільноті
7. **CWE (Common Weakness Enumerations)** - залишено з поясненням українською
